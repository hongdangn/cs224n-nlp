{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CfdXSVC4KMrP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import string\n",
        "import random\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAzIrc_EN3WA",
        "outputId": "7d5adb52-7871-4b08-ddb1-a1d6c0fc7002"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_FWa3ewTOB-w"
      },
      "outputs": [],
      "source": [
        "with open('drive/MyDrive/data/text8', 'r') as file:\n",
        "  content = file.read()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S6s6HRkOTTn"
      },
      "source": [
        "## Pre-processing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "P8psKVTtOOcE"
      },
      "outputs": [],
      "source": [
        "def clean_encode_data(text):\n",
        "  for punctuation in string.punctuation:\n",
        "    text = text.replace(punctuation, '')\n",
        "  text = text.strip()\n",
        "  split_text = text.split()\n",
        "  print(len(split_text))\n",
        "  word_and_index, index_and_word = {}, {}\n",
        "  limit, cnt = 10000, 0\n",
        "  for id, (word, count) in enumerate(Counter(split_text).items()):\n",
        "    if count > 5 and id < limit:\n",
        "      word_and_index[word] = cnt\n",
        "      index_and_word[cnt] = word\n",
        "      cnt += 1\n",
        "  encoded_text = [word_and_index[word] for word in word_and_index]\n",
        "  return encoded_text, word_and_index, index_and_word\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCxifoHf8I3n",
        "outputId": "8b3f25f7-8c3c-4506-a8bd-1174be2dcddb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17005207\n"
          ]
        }
      ],
      "source": [
        "encoded_text, word_and_index, index_and_word = clean_encode_data(content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mnaiVTf85Jl"
      },
      "source": [
        "## Skip-gram (negative sampling) model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "N--qmISnevRe"
      },
      "outputs": [],
      "source": [
        "class generate_data():\n",
        "  def __init__(self, window_size, text):\n",
        "    self.window_size = window_size\n",
        "    self.text = text\n",
        "\n",
        "  def get_target(self, word_id):\n",
        "    start = max(0, word_id - self.window_size)\n",
        "    end = min(len(self.text), word_id + self.window_size)\n",
        "    return self.text[start: word_id] + self.text[word_id + 1: end]\n",
        "\n",
        "  def make_dataset(self, num_samples):\n",
        "    dataset = []\n",
        "    for center in self.text:\n",
        "      targets = self.get_target(center)\n",
        "      for context in targets:\n",
        "        neg_samples = np.random.choice(self.text, num_samples)\n",
        "        neg_samples = [torch.tensor([sample]) for sample in neg_samples]\n",
        "        dataset.append((torch.tensor([center]), torch.tensor([context]), torch.tensor(neg_samples)))\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uTR48NEu84MR"
      },
      "outputs": [],
      "source": [
        "class skip_gram(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_size):\n",
        "    super(skip_gram, self).__init__()\n",
        "\n",
        "    self.center_embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "    self.context_embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "    self.center_embeddings.weight.data.uniform_(-1, 1)\n",
        "    self.context_embeddings.weight.data.uniform_(-1, 1)\n",
        "\n",
        "  def forward(self, center, context, neg_samples):\n",
        "    emb_center = self.center_embeddings(center)\n",
        "    emb_context = self.context_embeddings(context)\n",
        "    emb_negsamples = self.context_embeddings(neg_samples)\n",
        "\n",
        "    return emb_center, emb_context, emb_negsamples"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class loss_var(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, center_vector, context_vector, neg_vectors):\n",
        "    pos_loss = torch.mul(center_vector, context_vector).squeeze().sum().sigmoid().log()\n",
        "    neg_loss = torch.mul(-neg_vectors, center_vector).sum(dim = 1).sigmoid().log().sum()\n",
        "\n",
        "    return -(pos_loss + neg_loss)"
      ],
      "metadata": {
        "id": "ftK3VHtr34cr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phgexPrHNIv4"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpEQV3WreswJ",
        "outputId": "61925ef1-00e4-4148-f8a0-fc2812ac65fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(tensor([0]), tensor([1]), tensor([8514, 7633,  930, 1348,  809])),\n",
              " (tensor([0]), tensor([2]), tensor([3687, 3219, 2294, 3070, 6165])),\n",
              " (tensor([0]), tensor([3]), tensor([2434, 2931, 3062, 8835, 5068])),\n",
              " (tensor([1]), tensor([0]), tensor([2482, 5613, 2420, 8799, 3141])),\n",
              " (tensor([1]), tensor([2]), tensor([8859, 8259, 2460,  129, 5649]))]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "vocab_size = len(encoded_text)\n",
        "emb_size = 100\n",
        "neg_samples = 5\n",
        "\n",
        "model = skip_gram(vocab_size = vocab_size, embed_size = emb_size)\n",
        "loss = loss_var()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
        "\n",
        "gen_data = generate_data(window_size = 4, text = encoded_text)\n",
        "dataset = gen_data.make_dataset(num_samples = neg_samples)\n",
        "\n",
        "dataset[:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5MMw3daNvPo",
        "outputId": "abd53a27-b600-4544-ad28-279f4fbf2996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss 1.6876938343048096\n",
            "Epoch 1, loss 1.4038118124008179\n",
            "Epoch 2, loss 1.1912139654159546\n",
            "Epoch 3, loss 1.0296595096588135\n",
            "Epoch 4, loss 0.9045617580413818\n",
            "Epoch 5, loss 0.8056939840316772\n",
            "Epoch 6, loss 0.7259702682495117\n",
            "Epoch 7, loss 0.6604741215705872\n",
            "Epoch 8, loss 0.6057572364807129\n",
            "Epoch 9, loss 0.5593625903129578\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "device = 'cpu'\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for center, context, neg_samples in dataset:\n",
        "    center.to(device); context.to(device); neg_samples.to(device)\n",
        "    emb_center, emb_context, emb_negsamples = model(center, context, neg_samples)\n",
        "    loss_val = loss(emb_center, emb_context, emb_negsamples)\n",
        "    optimizer.zero_grad()\n",
        "    loss_val.backward()\n",
        "    optimizer.step()\n",
        "  if epoch % 1 == 0:\n",
        "    print(\"Epoch {}, loss {}\".format(epoch, loss_val.item()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute similarity and Test"
      ],
      "metadata": {
        "id": "dFUfww6rRcmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def words_similarity(word, model):\n",
        "  word_emb = model.center_embeddings(word)\n",
        "  word_emb /= torch.norm(word_emb, dim = 1).squeeze()\n",
        "\n",
        "  context_weights = model.context_embeddings.weight.data\n",
        "  context_weights /= torch.norm(context_weights, dim = 1).reshape()\n",
        "\n",
        "  shape_0, shape_1 = context_weights.shape\n",
        "  similarities = torch.matmul(word_emb, context_weights.reshape(shape_1, shape_0))\n",
        "  values, indexes = torch.topk(similarities, 5)\n",
        "  most_similar = [index_and_word[id] for id in indexes[0]]\n",
        "  return most_similar"
      ],
      "metadata": {
        "id": "9tqAddABRbBH"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = index_and_word[0]\n",
        "words_similarity(torch.tensor([0]), model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "d1ibJfztdUPq",
        "outputId": "213297ff-c52c-4166-93f4-62731ce8d22f"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (100) must match the size of tensor b (9145) at non-singleton dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-0e6fb7946435>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_and_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwords_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-57-a9d12033230d>\u001b[0m in \u001b[0;36mwords_similarity\u001b[0;34m(word, model)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mcontext_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mcontext_weights\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mshape_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (100) must match the size of tensor b (9145) at non-singleton dimension 1"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}