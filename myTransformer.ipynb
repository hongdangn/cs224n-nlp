{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.functional as F"
      ],
      "metadata": {
        "id": "GX3DWnwjVuWf"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encoding Layer"
      ],
      "metadata": {
        "id": "6TrxiM69bQr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, max_seq_length, d_model):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    positions = torch.arange(0, max_seq_length, dtype = torch.float).unsqueeze(0)\n",
        "    for dim in range(d_model):\n",
        "      std_dim = dim//2 * 2\n",
        "      dimVal = torch.tensor([(1/1e4)**(std_dim / d_model)] * max_seq_length).unsqueeze(0)\n",
        "      dimVal = torch.sin(dimVal * positions) if dim % 2 == 0 else torch.cos(dimVal * positions)\n",
        "      if dim == 0:\n",
        "        self.allDimVals = dimVal\n",
        "      else:\n",
        "        self.allDimVals = torch.cat((self.allDimVals, dimVal), dim = 0)\n",
        "    self.allDimVals = self.allDimVals.transpose(-1, -2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x + self.allDimVals[:x.size(1)]"
      ],
      "metadata": {
        "id": "kHFqu366z8T5"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Head Attention Layer"
      ],
      "metadata": {
        "id": "ggV1y98_bUFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, n_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    assert d_model % n_heads == 0\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "    self.d_k = d_model // n_heads\n",
        "\n",
        "    self.W_q = nn.Linear(d_model, d_model)\n",
        "    self.W_k = nn.Linear(d_model, d_model)\n",
        "    self.W_v = nn.Linear(d_model, d_model)\n",
        "    self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "  def scaleDotProduct(self, Q, K, V, mask = None):\n",
        "    attentionScore = torch.matmul(Q, K.transpose(-1, -2)) / torch.sqrt(self.d_k)\n",
        "    if mask is not None:\n",
        "      attentionScore = attentionScore.masked_fill_(mask == 0, -1e9)\n",
        "    attentionScore = torch.softmax(attentionScore, dim = -1)\n",
        "\n",
        "    output = torch.matmul(attentionScore, V)\n",
        "    return output\n",
        "\n",
        "  def split_heads(self, x):\n",
        "    batch_size, seq_length, d_model = x.size()\n",
        "    return x.view(batch_size, seq_length, self.n_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "  def combine_heads(self, x):\n",
        "    batch_size, n_heads, seq_length, d_k = x.size()\n",
        "    return x.transpose(1, 2).view(batch_size, seq_length, d_k * n_heads)\n",
        "\n",
        "  def forward(self, Q, K, V, mask):\n",
        "    Q = self.split_heads(self.W_q(Q))\n",
        "    K = self.split_heads(self.W_k(K))\n",
        "    V = self.split_heads(self.W_v(V))\n",
        "\n",
        "    attentionMat = self.scaleDotProduct(Q, K, V, mask)\n",
        "    output = self.W_o(self.combine_heads(attentionMat))\n",
        "    return output"
      ],
      "metadata": {
        "id": "pBE2iYBiZFv7"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feed Forward Layer (after Add&Norm Layer)"
      ],
      "metadata": {
        "id": "y_66Y1_EbXuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, d_model, d_fflayer):\n",
        "    super(FeedForward, self).__init__()\n",
        "    self.fc1 = nn.Linear(d_model, d_fflayer)\n",
        "    self.fc2 = nn.Linear(d_fflayer, d_model)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.fc2(x)\n",
        "    return self.relu(x)"
      ],
      "metadata": {
        "id": "je7LwiuAY2cA"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder Layer"
      ],
      "metadata": {
        "id": "pDhQEnpUb8sq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, d_fflayer, max_seq_length, dropout, n_heads):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.attention = MultiHeadAttention(d_model, n_heads)\n",
        "    self.ff = FeedForward(d_model, d_fflayer)\n",
        "    self.norm = nn.LayerNorm(d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    attentionScore = self.attention(x, x, x, mask)\n",
        "    x = self.norm(x + self.dropout(attentionScore))\n",
        "    ff_output = self.ff(x)\n",
        "    x = self.norm(x + self.dropout(ff_output))\n",
        "    return x"
      ],
      "metadata": {
        "id": "DSbiLhwDcAFt"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder Layer"
      ],
      "metadata": {
        "id": "a2wE_ecRg_A6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, d_fflayer, dropout, n_heads):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.in_attention = MultiHeadAttention(d_model, n_heads)\n",
        "    self.cross_attention = MultiHeadAttention(d_model, n_heads)\n",
        "    self.ff = FeedForward(d_model, d_fflayer)\n",
        "    self.norm = nn.LayerNorm(d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "    in_attentionScore = self.in_attention(x, x, x, tgt_mask)\n",
        "    x = self.norm(x + self.dropout(in_attentionScore))\n",
        "    cross_attentionScore = self.cross_attention(x, enc_output, enc_output, src_mask)\n",
        "    x = self.norm(x + self.dropout(cross_attentionScore))\n",
        "    ffoutput = self.ff(x)\n",
        "    x = self.norm(x + self.dropout(ffoutput))\n",
        "    return x"
      ],
      "metadata": {
        "id": "7Ju6wJXBhF7o"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Model"
      ],
      "metadata": {
        "id": "Gdot5Z6gowW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, d_model, d_fflayer, max_seq_length, dropout, n_layers, n_heads, src_vocab_size, tgt_vocab_size):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    self.embed1 = nn.Embedding(src_vocab_size, d_model)\n",
        "    self.embed2 = nn.Embedding(tgt_vocab_size, d_model)\n",
        "    self.pe = PositionalEncoding(max_seq_length, d_model)\n",
        "    self.encoders = nn.ModuleList([EncoderLayer(d_model, d_fflayer, max_seq_length, dropout, n_heads)] * n_layers)\n",
        "    self.decoders = nn.ModuleList([DecoderLayer(d_model, d_fflayer, dropout, n_heads)] * n_layers)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "  def generate(self, src, tgt):\n",
        "    src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "    tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
        "    seq_length = tgt.size(1)\n",
        "    peak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal = 1)).bool()\n",
        "    tgt_mask = tgt_mask & peak_mask\n",
        "    return src_mask, tgt_mask\n",
        "\n",
        "  def forward(self, x, src, tgt):\n",
        "    src_mask, tgt_mask = self.generate(src, tgt)\n",
        "    src, tgt = self.embed1(src), self.embed2(tgt)\n",
        "    src_embed, tgt_embed = self.dropout(self.pe(src)), self.dropout(self.pe(tgt))\n",
        "    for enc_layer in self.encoders:\n",
        "      src_embed = enc_layer(src_embed, src_mask)\n",
        "\n",
        "    for dec_layer in self.decoders:\n",
        "      tgt_embed = dec_layer(tgt_embed, src_embed, src_mask, tgt_mask)\n",
        "    output = self.fc(tgt_embed)\n",
        "    return output"
      ],
      "metadata": {
        "id": "vYKKfOOYoyu5"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "K_aeU8QH2xkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab_size = 5000\n",
        "tgt_vocab_size = 5000\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "max_seq_length = 100\n",
        "dropout = 0.1\n",
        "\n",
        "transformer = Transformer(d_model, d_fflayer, max_seq_length, dropout, n_layers, n_heads, src_vocab_size, tgt_vocab_size)\n",
        "\n",
        "# Generate random sample data\n",
        "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
        "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "transformer.train()\n",
        "\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    output = transformer(src_data, tgt_data[:, :-1])\n",
        "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "EKDduxa52zOm"
      },
      "execution_count": 75,
      "outputs": []
    }
  ]
}